# Programmation parallèle

En informatique, le **parallélisme** consiste à mettre en œuvre des architectures informatiques 
permettant de traiter des informations ***de manière simultanée***, ainsi que des algorithmes spécialisés pour celles-ci. 
L'objectif du parallélisme consiste à améliorer l'efficacité des algorithmes. L'idée étant de ***distribuer le calcul sur plusieurs processeurs 
qui éxécutent en même temps une sous partie du problème***, et par conséquent de réduire le temps d'exécution.

Les architectures parallèles sont devenues le paradigme dominant pour tous les ordinateurs depuis les années 2000. 
En effet, la vitesse de traitement qui est liée à l'augmentation de la fréquence des processeurs connait des limites. 
La création de processeurs multi-cœurs, traitant plusieurs instructions en même temps au sein du même composant, 
résout ce dilemme pour les machines de bureau depuis le milieu des années 2000.

Le calcul parallèle est incontournable dans certains domaines : 
- la dynamique des fluides, 
- les prédictions météorologiques, 
- la modélisation et simulation de problèmes de dimensions plus grandes, 
- le traitement de l'information et l'exploration de grandes masses de données, 
- le décryptage de messages, 
- la recherche de mots de passe, 
- le traitement d'images ou la fabrication d'images de synthèse,
- l'intelligence artificielle 
- la fabrication automatisée. 

Initialement, c'est dans le domaine des supercalculateurs que le parallélisme a été utilisé, à des fins scientifiques.

Source: Adapté de https://fr.wikipedia.org/wiki/Parall%C3%A9lisme_(informatique)


Bonne lecture !
